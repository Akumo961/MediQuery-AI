{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "# MediQuery AI - Data Exploration and Analysis\n",
    "\n",
    "This notebook explores medical datasets and performs initial analysis for our multimodal healthcare AI system.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä MediQuery AI - Data Exploration Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ## 1. Dataset Overview\n",
    "\n",
    "# Create sample medical data for demonstration\n",
    "def create_sample_medical_data():\n",
    "    \"\"\"Create sample medical datasets for exploration\"\"\"\n",
    "\n",
    "    # Sample PubMed papers data\n",
    "    papers_data = {\n",
    "        'title': [\n",
    "            'AI in Medical Imaging: Current Applications and Future Prospects',\n",
    "            'Deep Learning for Electronic Health Records Analysis',\n",
    "            'Natural Language Processing in Clinical Decision Support',\n",
    "            'Computer Vision Applications in Radiology Diagnostics',\n",
    "            'Multimodal AI Systems for Healthcare: A Comprehensive Review',\n",
    "            'Machine Learning in Drug Discovery and Development',\n",
    "            'Federated Learning for Healthcare Data Privacy',\n",
    "            'Transformer Models for Medical Text Analysis',\n",
    "            'Automated Medical Image Segmentation Using CNNs',\n",
    "            'AI-Powered Clinical Trial Optimization Strategies'\n",
    "        ],\n",
    "        'journal': [\n",
    "            'Nature Medicine', 'JAMA', 'New England Journal of Medicine',\n",
    "            'Radiology', 'Nature Digital Medicine', 'Science Translational Medicine',\n",
    "            'The Lancet Digital Health', 'Journal of Medical Internet Research',\n",
    "            'Medical Image Analysis', 'Nature Biotechnology'\n",
    "        ],\n",
    "        'year': [2024, 2023, 2024, 2023, 2024, 2023, 2024, 2023, 2024, 2023],\n",
    "        'citations': [156, 89, 203, 134, 245, 112, 78, 167, 91, 188],\n",
    "        'impact_factor': [53.4, 51.3, 91.2, 7.9, 14.8, 17.1, 23.1, 5.4, 8.9, 43.1],\n",
    "        'category': [\n",
    "            'Medical Imaging', 'EHR Analysis', 'Clinical NLP', 'Radiology AI',\n",
    "            'Multimodal AI', 'Drug Discovery', 'Privacy', 'Text Analysis',\n",
    "            'Image Segmentation', 'Clinical Trials'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Sample medical images metadata\n",
    "    images_data = {\n",
    "        'image_id': [f'IMG_{i:04d}' for i in range(100)],\n",
    "        'modality': np.random.choice(['X-ray', 'MRI', 'CT', 'Ultrasound'], 100),\n",
    "        'body_part': np.random.choice(['Chest', 'Brain', 'Abdomen', 'Heart', 'Lung'], 100),\n",
    "        'resolution': np.random.choice(['512x512', '1024x1024', '256x256'], 100),\n",
    "        'file_size_mb': np.random.uniform(0.5, 15.0, 100).round(2),\n",
    "        'quality_score': np.random.uniform(0.6, 1.0, 100).round(3),\n",
    "        'has_annotation': np.random.choice([True, False], 100),\n",
    "        'diagnosis': np.random.choice(['Normal', 'Abnormal', 'Uncertain'], 100)\n",
    "    }\n",
    "\n",
    "    # Sample clinical data\n",
    "    clinical_data = {\n",
    "        'patient_id': [f'PT_{i:05d}' for i in range(500)],\n",
    "        'age': np.random.randint(18, 90, 500),\n",
    "        'gender': np.random.choice(['Male', 'Female'], 500),\n",
    "        'diagnosis_category': np.random.choice([\n",
    "            'Cardiovascular', 'Respiratory', 'Neurological',\n",
    "            'Oncology', 'Orthopedic', 'Endocrine'\n",
    "        ], 500),\n",
    "        'treatment_duration_days': np.random.randint(1, 365, 500),\n",
    "        'outcome': np.random.choice(['Improved', 'Stable', 'Deteriorated'], 500),\n",
    "        'ai_assisted': np.random.choice([True, False], 500)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(papers_data), pd.DataFrame(images_data), pd.DataFrame(clinical_data)\n",
    "\n",
    "# Generate sample datasets\n",
    "papers_df, images_df, clinical_df = create_sample_medical_data()\n",
    "\n",
    "print(\"üìö Literature Dataset:\")\n",
    "print(f\"  ‚Ä¢ Papers: {len(papers_df)}\")\n",
    "print(f\"  ‚Ä¢ Journals: {papers_df['journal'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Years: {papers_df['year'].min()}-{papers_df['year'].max()}\")\n",
    "print()\n",
    "\n",
    "print(\"üñºÔ∏è Medical Images Dataset:\")\n",
    "print(f\"  ‚Ä¢ Images: {len(images_df)}\")\n",
    "print(f\"  ‚Ä¢ Modalities: {list(images_df['modality'].unique())}\")\n",
    "print(f\"  ‚Ä¢ Body Parts: {list(images_df['body_part'].unique())}\")\n",
    "print()\n",
    "\n",
    "print(\"üè• Clinical Dataset:\")\n",
    "print(f\"  ‚Ä¢ Patients: {len(clinical_df)}\")\n",
    "print(f\"  ‚Ä¢ Age Range: {clinical_df['age'].min()}-{clinical_df['age'].max()}\")\n",
    "print(f\"  ‚Ä¢ Diagnoses: {list(clinical_df['diagnosis_category'].unique())}\")\n",
    "print()\n",
    "\n",
    "# ## 2. Literature Analysis\n",
    "\n",
    "def analyze_literature_trends():\n",
    "    \"\"\"Analyze trends in medical literature\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('üìä Medical Literature Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Publications by year\n",
    "    year_counts = papers_df['year'].value_counts().sort_index()\n",
    "    axes[0, 0].bar(year_counts.index, year_counts.values, color='steelblue', alpha=0.8)\n",
    "    axes[0, 0].set_title('Publications by Year')\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Number of Papers')\n",
    "\n",
    "    # Citations vs Impact Factor\n",
    "    axes[0, 1].scatter(papers_df['impact_factor'], papers_df['citations'],\n",
    "                      c=papers_df['year'], cmap='viridis', alpha=0.7, s=60)\n",
    "    axes[0, 1].set_title('Citations vs Journal Impact Factor')\n",
    "    axes[0, 1].set_xlabel('Impact Factor')\n",
    "    axes[0, 1].set_ylabel('Citations')\n",
    "\n",
    "    # Research categories\n",
    "    category_counts = papers_df['category'].value_counts()\n",
    "    axes[1, 0].barh(category_counts.index, category_counts.values, color='coral', alpha=0.8)\n",
    "    axes[1, 0].set_title('Research Categories')\n",
    "    axes[1, 0].set_xlabel('BLEU Score')\n",
    "    axes[1, 0].set_ylabel('ROUGE-L Score')\n",
    "\n",
    "    # Training time comparison\n",
    "    axes[1, 1].barh(qa_df['model_name'], qa_df['training_time_hrs'], color='gold', alpha=0.8)\n",
    "    axes[1, 1].set_title('Training Time Comparison')\n",
    "    axes[1, 1].set_xlabel('Training Time (hours)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_qa_models()\n",
    "\n",
    "# ## 5. Cross-Model Performance Analysis\n",
    "\n",
    "def cross_model_analysis():\n",
    "    \"\"\"Compare performance across different model types\"\"\"\n",
    "\n",
    "    print(\"\\nüî¨ Cross-Model Performance Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Best performing models\n",
    "    best_text_model = text_df.loc[text_df['accuracy'].idxmax()]\n",
    "    best_image_model = image_df.loc[image_df['accuracy'].idxmax()]\n",
    "    best_qa_model = qa_df.loc[qa_df['f1_score'].idxmax()]\n",
    "\n",
    "    print(f\"\\nüèÜ Best Performing Models:\")\n",
    "    print(f\"  ‚Ä¢ Text Classification: {best_text_model['model_name']} (Acc: {best_text_model['accuracy']:.3f})\")\n",
    "    print(f\"  ‚Ä¢ Image Classification: {best_image_model['model_name']} (Acc: {best_image_model['accuracy']:.3f})\")\n",
    "    print(f\"  ‚Ä¢ Question Answering: {best_qa_model['model_name']} (F1: {best_qa_model['f1_score']:.3f})\")\n",
    "\n",
    "    # Efficiency analysis\n",
    "    print(f\"\\n‚ö° Efficiency Leaders:\")\n",
    "    fastest_text = text_df.loc[text_df['inference_time_ms'].idxmin()]\n",
    "    fastest_image = image_df.loc[image_df['inference_time_ms'].idxmin()]\n",
    "    fastest_qa = qa_df.loc[qa_df['inference_time_ms'].idxmin()]\n",
    "\n",
    "    print(f\"  ‚Ä¢ Fastest Text Model: {fastest_text['model_name']} ({fastest_text['inference_time_ms']}ms)\")\n",
    "    print(f\"  ‚Ä¢ Fastest Image Model: {fastest_image['model_name']} ({fastest_image['inference_time_ms']}ms)\")\n",
    "    print(f\"  ‚Ä¢ Fastest QA Model: {fastest_qa['model_name']} ({fastest_qa['inference_time_ms']}ms)\")\n",
    "\n",
    "    # Training efficiency\n",
    "    print(f\"\\nüìö Training Efficiency:\")\n",
    "    efficient_text = text_df.loc[(text_df['accuracy'] / text_df['training_time_hrs']).idxmax()]\n",
    "    efficient_image = image_df.loc[(image_df['accuracy'] / image_df['training_time_hrs']).idxmax()]\n",
    "    efficient_qa = qa_df.loc[(qa_df['f1_score'] / qa_df['training_time_hrs']).idxmax()]\n",
    "\n",
    "    print(f\"  ‚Ä¢ Most Efficient Text: {efficient_text['model_name']}\")\n",
    "    print(f\"  ‚Ä¢ Most Efficient Image: {efficient_image['model_name']}\")\n",
    "    print(f\"  ‚Ä¢ Most Efficient QA: {efficient_qa['model_name']}\")\n",
    "\n",
    "cross_model_analysis()\n",
    "\n",
    "# ## 6. Confusion Matrix Analysis\n",
    "\n",
    "def generate_confusion_matrices():\n",
    "    \"\"\"Generate confusion matrices for model evaluation\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('üéØ Confusion Matrix Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Mock confusion matrices\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Text classification confusion matrix (4 classes)\n",
    "    text_classes = ['Research Paper', 'Clinical Note', 'Drug Info', 'Diagnostic Report']\n",
    "    text_cm = np.array([[145, 8, 3, 2], [5, 138, 7, 1], [2, 4, 142, 6], [1, 2, 5, 147]])\n",
    "\n",
    "    sns.heatmap(text_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=text_classes, yticklabels=text_classes, ax=axes[0])\n",
    "    axes[0].set_title('Text Classification\\n(Document Types)')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "\n",
    "    # Image classification confusion matrix (4 modalities)\n",
    "    image_classes = ['X-ray', 'MRI', 'CT', 'Ultrasound']\n",
    "    image_cm = np.array([[89, 3, 2, 1], [2, 91, 4, 0], [1, 5, 87, 2], [0, 1, 3, 92]])\n",
    "\n",
    "    sns.heatmap(image_cm, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=image_classes, yticklabels=image_classes, ax=axes[1])\n",
    "    axes[1].set_title('Image Classification\\n(Modality Types)')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "\n",
    "    # QA performance by question type\n",
    "    qa_classes = ['Definition', 'Procedure', 'Diagnosis', 'Treatment']\n",
    "    qa_scores = np.array([[0.82, 0.15, 0.02, 0.01], [0.08, 0.78, 0.12, 0.02],\n",
    "                         [0.03, 0.09, 0.85, 0.03], [0.01, 0.05, 0.08, 0.86]])\n",
    "\n",
    "    sns.heatmap(qa_scores, annot=True, fmt='.2f', cmap='Greens',\n",
    "                xticklabels=qa_classes, yticklabels=qa_classes, ax=axes[2])\n",
    "    axes[2].set_title('QA Performance\\n(Question Types)')\n",
    "    axes[2].set_xlabel('Predicted Type')\n",
    "    axes[2].set_ylabel('Actual Type')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_confusion_matrices()\n",
    "\n",
    "# ## 7. Model Robustness Testing\n",
    "\n",
    "def robustness_analysis():\n",
    "    \"\"\"Analyze model robustness across different conditions\"\"\"\n",
    "\n",
    "    print(\"\\nüõ°Ô∏è Model Robustness Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Simulate robustness testing results\n",
    "    np.random.seed(42)\n",
    "\n",
    "    conditions = ['Clean Data', 'Noisy Data', 'Low Quality', 'Domain Shift', 'Adversarial']\n",
    "\n",
    "    # Text model robustness\n",
    "    text_robustness = {\n",
    "        'BioBERT': [0.924, 0.887, 0.854, 0.798, 0.723],\n",
    "        'PubMedBERT': [0.937, 0.901, 0.867, 0.812, 0.745],\n",
    "        'ClinicalBERT': [0.911, 0.876, 0.841, 0.789, 0.708]\n",
    "    }\n",
    "\n",
    "    # Image model robustness\n",
    "    image_robustness = {\n",
    "        'Vision Transformer': [0.941, 0.889, 0.823, 0.756, 0.634],\n",
    "        'EfficientNet-B4': [0.923, 0.878, 0.831, 0.782, 0.687],\n",
    "        'ResNet50': [0.887, 0.845, 0.798, 0.743, 0.661]\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('üõ°Ô∏è Model Robustness Under Different Conditions', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Text model robustness\n",
    "    for model, scores in text_robustness.items():\n",
    "        axes[0].plot(conditions, scores, marker='o', linewidth=2, label=model)\n",
    "\n",
    "    axes[0].set_title('Text Models Robustness')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Image model robustness\n",
    "    for model, scores in image_robustness.items():\n",
    "        axes[1].plot(conditions, scores, marker='s', linewidth=2, label=model)\n",
    "\n",
    "    axes[1].set_title('Image Models Robustness')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print robustness summary\n",
    "    print(\"\\nüìä Robustness Summary:\")\n",
    "    print(\"  ‚Ä¢ All models show degradation under adverse conditions\")\n",
    "    print(\"  ‚Ä¢ Vision Transformer most robust to clean data\")\n",
    "    print(\"  ‚Ä¢ PubMedBERT shows best text robustness overall\")\n",
    "    print(\"  ‚Ä¢ Adversarial examples cause 20-30% performance drop\")\n",
    "\n",
    "robustness_analysis()\n",
    "\n",
    "# ## 8. Computational Requirements\n",
    "\n",
    "def analyze_computational_requirements():\n",
    "    \"\"\"Analyze computational requirements of different models\"\"\"\n",
    "\n",
    "    # Mock computational data\n",
    "    comp_data = {\n",
    "        'Model Type': ['Text (BioBERT)', 'Text (PubMedBERT)', 'Image (ViT)', 'Image (EfficientNet)', 'QA (RoBERTa)'],\n",
    "        'Parameters (M)': [110, 110, 86, 19, 125],\n",
    "        'Memory (GB)': [1.2, 1.2, 2.1, 0.8, 1.4],\n",
    "        'GPU Hours': [2.3, 2.7, 8.3, 6.1, 5.1],\n",
    "        'Inference (ms)': [45, 52, 67, 31, 78],\n",
    "        'Energy (kWh)': [4.2, 4.8, 15.6, 11.2, 9.3]\n",
    "    }\n",
    "\n",
    "    comp_df = pd.DataFrame(comp_data)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('üíª Computational Requirements Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Parameters vs Memory\n",
    "    scatter = axes[0, 0].scatter(comp_df['Parameters (M)'], comp_df['Memory (GB)'],\n",
    "                                s=100, alpha=0.7, c=comp_df['GPU Hours'], cmap='viridis')\n",
    "    axes[0, 0].set_title('Model Size vs Memory Usage')\n",
    "    axes[0, 0].set_xlabel('Parameters (Millions)')\n",
    "    axes[0, 0].set_ylabel('Memory (GB)')\n",
    "    plt.colorbar(scatter, ax=axes[0, 0], label='GPU Hours')\n",
    "\n",
    "    # Training time vs model type\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "    axes[0, 1].bar(comp_df['Model Type'], comp_df['GPU Hours'], color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_title('Training Time by Model Type')\n",
    "    axes[0, 1].set_ylabel('GPU Hours')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Inference speed comparison\n",
    "    axes[1, 0].barh(comp_df['Model Type'], comp_df['Inference (ms)'], color='lightcoral', alpha=0.8)\n",
    "    axes[1, 0].set_title('Inference Speed Comparison')\n",
    "    axes[1, 0].set_xlabel('Inference Time (ms)')\n",
    "\n",
    "    # Energy consumption\n",
    "    axes[1, 1].pie(comp_df['Energy (kWh)'], labels=comp_df['Model Type'], autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Energy Consumption Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n‚ö° Computational Insights:\")\n",
    "    print(f\"  ‚Ä¢ Most Parameter-Efficient: {comp_df.loc[comp_df['Parameters (M)'].idxmin(), 'Model Type']}\")\n",
    "    print(f\"  ‚Ä¢ Fastest Inference: {comp_df.loc[comp_df['Inference (ms)'].idxmin(), 'Model Type']}\")\n",
    "    print(f\"  ‚Ä¢ Most Memory Efficient: {comp_df.loc[comp_df['Memory (GB)'].idxmin(), 'Model Type']}\")\n",
    "    print(f\"  ‚Ä¢ Lowest Energy Usage: {comp_df.loc[comp_df['Energy (kWh)'].idxmin(), 'Model Type']}\")\n",
    "\n",
    "analyze_computational_requirements()\n",
    "\n",
    "# ## 9. Model Recommendations\n",
    "\n",
    "def generate_model_recommendations():\n",
    "    \"\"\"Generate recommendations for different use cases\"\"\"\n",
    "\n",
    "    print(\"\\nüéØ Model Recommendations by Use Case\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    recommendations = {\n",
    "        \"High Accuracy Requirements\": {\n",
    "            \"Text\": \"PubMedBERT (93.7% accuracy)\",\n",
    "            \"Image\": \"Vision Transformer (94.1% accuracy)\",\n",
    "            \"QA\": \"PubMedBERT-QA (83.1% F1)\",\n",
    "            \"Rationale\": \"Best overall performance, suitable for critical applications\"\n",
    "        },\n",
    "\n",
    "        \"Real-time Applications\": {\n",
    "            \"Text\": \"SciBERT (35ms inference)\",\n",
    "            \"Image\": \"ResNet50 (23ms inference)\",\n",
    "            \"QA\": \"BioBERT-QA (71ms inference)\",\n",
    "            \"Rationale\": \"Optimized for speed, acceptable accuracy trade-off\"\n",
    "        },\n",
    "\n",
    "        \"Resource Constrained\": {\n",
    "            \"Text\": \"ClinicalBERT (1.8hr training)\",\n",
    "            \"Image\": \"DenseNet121 (7.9M parameters)\",\n",
    "            \"QA\": \"BioBERT-QA (4.7hr training)\",\n",
    "            \"Rationale\": \"Efficient training and deployment, lower compute requirements\"\n",
    "        },\n",
    "\n",
    "        \"Production Deployment\": {\n",
    "            \"Text\": \"BioBERT (balanced performance)\",\n",
    "            \"Image\": \"EfficientNet-B4 (good accuracy/speed balance)\",\n",
    "            \"QA\": \"RoBERTa-BioASQ (proven track record)\",\n",
    "            \"Rationale\": \"Reliable, well-tested, good performance-efficiency balance\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for use_case, models in recommendations.items():\n",
    "        print(f\"\\nüîπ {use_case}:\")\n",
    "        print(f\"  ‚Ä¢ Text: {models['Text']}\")\n",
    "        print(f\"  ‚Ä¢ Image: {models['Image']}\")\n",
    "        print(f\"  ‚Ä¢ QA: {models['QA']}\")\n",
    "        print(f\"  ‚Ä¢ Why: {models['Rationale']}\")\n",
    "\n",
    "generate_model_recommendations()\n",
    "\n",
    "# ## 10. Export Evaluation Results\n",
    "\n",
    "def export_evaluation_results():\n",
    "    \"\"\"Export model evaluation results\"\"\"\n",
    "\n",
    "    # Combine all results\n",
    "    evaluation_results = {\n",
    "        'evaluation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'text_classification': text_df.to_dict('records'),\n",
    "        'image_classification': image_df.to_dict('records'),\n",
    "        'question_answering': qa_df.to_dict('records'),\n",
    "        'best_models': {\n",
    "            'text': text_df.loc[text_df['accuracy'].idxmax()].to_dict(),\n",
    "            'image': image_df.loc[image_df['accuracy'].idxmax()].to_dict(),\n",
    "            'qa': qa_df.loc[qa_df['f1_score'].idxmax()].to_dict()\n",
    "        },\n",
    "        'summary_metrics': {\n",
    "            'avg_text_accuracy': text_df['accuracy'].mean(),\n",
    "            'avg_image_accuracy': image_df['accuracy'].mean(),\n",
    "            'avg_qa_f1': qa_df['f1_score'].mean(),\n",
    "            'total_models_evaluated': len(text_df) + len(image_df) + len(qa_df)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    import json\n",
    "    with open('../results/model_evaluation_results.json', 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "    print(\"\\nüíæ Evaluation Results Exported!\")\n",
    "    print(\"Results saved to: ../results/model_evaluation_results.json\")\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "evaluation_results = export_evaluation_results()\n",
    "\n",
    "print(\"\\n‚úÖ Model Evaluation Complete!\")\n",
    "print(\"Key Findings:\")\n",
    "print(\"  ‚Ä¢ PubMedBERT leads text classification (93.7% accuracy)\")\n",
    "print(\"  ‚Ä¢ Vision Transformer best for images (94.1% accuracy)\")\n",
    "print(\"  ‚Ä¢ RoBERTa-BioASQ excels in QA (83.1% F1 score)\")\n",
    "print(\"  ‚Ä¢ Trade-offs exist between accuracy and efficiency\")"
   ],
   "id": "a4973867b14858d0"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
